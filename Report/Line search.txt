In order to find the right step size which minimize the loss function, a line search has to be implemented.
The step size alpha is nothing more than a scalar: the learning rate for the conjugate gradient algorithm, which tells how far is right to move along a given direction. 
So, fixed the values of the weights W and the descent direction d, the main goal is to find the right value for alpha that is able to minimize the loss function defined as E(W+alpha*d).

Of course, we have to deal with a tradeoff: we want a good reduction, but we can't spend too much time computing the exact value for the optimum solution. So, the smarter way to get it is to use an inexact linesearch, that try some candidate step size and accepts the first one satisfying some conditions.  

This search is performed in two phases: 
-a bracketing phase, that finds an initial interval containing a minimizer;
-an interpolation phase that, given the interval, finds the right step length in it.

We decided to use one of the most popular line search condition: the Armijo-Wolfe condition.

The search for the right alpha is led by two condition:
-The Armijo one: which ensure that alpha gives a sufficient decrease of the objective function, being this reduction proportional to the step length alpha and the direction derivative gradient fTpK*.
The constant sigma1 has been set sigma1=10-4, since it is suggested in literature to be quite small.

-The Strong Wolfe condition: which garantees to choose steps whose size is not too small.
It is also known as Curvature condition and ensures that, moving of a step alpha along the given direction, the slope of our function if greater than sigma2 times the gradient phi(0) (if the slope is only slightly negative, the function cannot decrease rapidly along that direction, so it's better to stop the search).
In this case, the constant sigma2 is equal to 0.1, since a smaller value gives a more accurate line search.
Futhermore, having choosen the strong condition, which doesn't allow the derivative to be too positive, we are sure that the alpha found lies close to a stationary point of the function.

The algorithm satisfing the Strong Wolfe conditions is implemented through three functions, as described in the pseudocodes.
Since two consecutive values may be similar in finite-precision arithmetic, we set a threshold which garantees that the algorithm stops if two values of alpha are too close.

The first one try to find and return a good alpha; if it fails, it returns an interval in which continue the searching, invoking a function called Zoom, which decreases the size of the interval, until it finds and returns a good step length..
Zoom invokes another function, Interpolate_alpha, which is nothing more than the implementation of a bisection interpolation in order to find a trial alpha inside the given interval. 


