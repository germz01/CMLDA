\chapter{Experiments} % (fold)
\label{cha:experiments}
    In this final chapter we present the results we obtained by applying our model to the datasets we have used
    to validate and test our ANN, namely, MONKS and CUP. Other than the results, we also present some details
    about the validation phase for each one of the datasets. We enrich the presentation by adding some graphs
    of the ANN's performances during the experimental phase.

    \section{MONKS} % (fold)
    \label{sec:monks}
        Before delving into the details of the results we obtained by applying our model to the dataset, we
        provide some informations about the \textit{preprocessing routines} and \textit{validation schema} we
        decided to use. Here are the steps we followed in order to reach the final states of our analysis.

        \begin{enumerate}
            \item Since the MONKS datasets’ feature are categorical, that is, every feature’s value represents
            a class, not a numerical value, we preprocessed the three datasets by writing a script
            for applying a \textit{1-of-k encoding}, hence obtaining 17 binary input features.
            \item As a supplementary preprocessing phase, we have applied a \textit{symmetrization} to the
            matrix containing the dataset’s values, in order to ease the training during the validation phase
            by having a matrix of values closer to the symmetric behavior of the sigmoid function, which was
            introduced in section \ref{sec:the_activation_functions}.
            \item Since we have chosen to follow \cite{Bergstra:2012:RSH:2188385.2188395} for the
            hyperparameters' search during the validation phase, we first performed some
            \textit{preliminary trials} in order to have a glimpse on the best intervals for searching our
            model's hyperparameters. During this trials we manually varied the model's hyperparameters, e.g.
            the learning rate, the momentum constant and so on for the SGD and the rho constant for the CGD,
            and observed the resulting \textit{learning curves}. For this part of the analysis we have used
            the $20\%$ of the training set as validation set, and the remaining part for training the network.
            \item We then deepen the search using the most interesting intervals discovered during the
            preliminary trials in the validation phase by using our implementation of the (random)
            \textit{grid search algorithm}, in which we also used our implementation of the
            \textit{k-fold cross validation algorithm} (which follows the standard approach of using a value
            of 3 for the k parameter).
        \end{enumerate}

        Our validation schema for the MONKS dataset essentially consists in using the random grid
        search
        algorithm to investigate some random sampled "points" in the hyperparameters' space, evaluating the
        performances for each one of this points and finally selecting the best combinations of parameters
        based
        on the diffent metrics like \textit{generalization error}, \textit{accuracy}, \textit{precision},
        \textit{recall} and \textit{f1-score}. Both in table \ref{tab:monks_sgd} and table
        \ref{tab:monks_cgd} we can find the results for the application of our model to the MONKS dataset,
        using as optimizer, respectively, the Stochastic Gradient Descent and the Conjugate Gradient Method,
        represented in a succinct fashion. Each row correspond to a specific dataset. The values for both the
        MSE and the Accuracy are represented by taking the mean over 10 executions on each dataset using the
        the final configurations for the hyperparameters obtained during the validation phase.

        \subsection{Results for Stochastic Gradient Descent} % (fold)
        \label{sub:results_for_stochastic_gradient_descent}

            \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c | c |}
                            \hline
                            Task & Topology & Batch size & Activation & $\eta$ & $\alpha$ & $\lambda$
                            & MSE (TR - TS) & Accuracy (TR - TS) (\%) \\
                            \hline
                            MONK 1 & 17 -> 4 -> 8 -> 1 & batch & sigmoid & 0.61 & 0.83 & 0.0 & 0.018 - 0.026 &
                            97 \% - 95 \% \\
                            \hline
                            MONK 2 & 17 -> 4 -> 8 -> 1 & batch & sigmoid & 0.64 & 0.78 & 0.0 & 0.010 - 0.015 &
                            98 \% - 97 \% \\
                            \hline
                            MONK 3 & 17 -> 4 -> 8 -> 1 & batch & sigmoid & 0.66 & 0.86 & 0.0072
                            & 0.015 - 0.017 & 97 \% - 97 \% \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Results for the Stochastic Gradient Descent.}
                \label{tab:monks_sgd}
            \end{table}

            For efficiency related reasons we have decided to use the \textit{Nesterov momentum}, as
            described in \cite{Goodfellow-et-al-2016,Sutskever:2013:IIM:3042817.3043064}, both in the
            validation and testing phases. The regularization constant $\lambda$ is used only in the third
            dataset, since Monk 3 is the only one among the three that has noisy samples.

        % subsection results_for_stochastic_gradient_descent (end)

        \subsection{Results for Conjugate Gradient Method} % (fold)
        \label{sub:results_for_conjugate_gradient_method}

        \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c | c |}
                            \hline
                            Task & Topology & Activation & $\beta$ & $\sigma_{1}$ & $\sigma_{2}$ & $\rho$
                            & MSE (TR - TS) & Accuracy (TR - TS) (\%) \\
                            \hline
                            MONK 1 & 17 -> 4 -> 8 -> 1 & sigmoid & MHS & 0.0001 & 0.27 & 0.67
                            & $9.84e^{-5}$ - 0.0021 & 100 \% - 100 \% \\
                            \hline
                            MONK 2 & 17 -> 4 -> 8 -> 1 & sigmoid & MHS & 0.0001 & 0.12 & 0.73
                            & $9.62e^{-5}$ - 0.0026 & 100 \% - 99 \% \\
                            \hline
                            MONK 3 & 17 -> 4 -> 8 -> 1 & sigmoid & MHS & 0.0001 & 0.24 & 0.27
                            & 0.0038 - 0.010 & 99 \% - 98 \% \\
                            \hline
                            MONK 1 & 17 -> 4 -> 8 -> 1 & sigmoid & HS & 0.0001 & 0.28 & 0.0
                            & $9.94e^{-5}$ - 0.00059 & 100 \% - 100 \% \\
                            \hline
                            MONK 2 & 17 -> 4 -> 8 -> 1 & sigmoid & HS & 0.0001 & 0.35 & 0.0
                            & $9.85e^{-5}$ - 0.0019 & 100 \% - 100 \% \\
                            \hline
                            MONK 3 & 17 -> 4 -> 8 -> 1 & sigmoid & HS & 0.0001 & 0.14 & 0.0
                            & 0.0033 - 0.011 & 99 \% - 98 \% \\
                            \hline
                            MONK 1 & 17 -> 4 -> 8 -> 1 & sigmoid & PR & 0.0001 & 0.11 & 0.0
                            & 0.014 - 0.030 & 97 \% - 94 \% \\
                            \hline
                            MONK 2 & 17 -> 4 -> 8 -> 1 & sigmoid & PR & 0.0001 & 0.21 & 0.0
                            & 0.050 - 0.059 & 97 \% - 88 \% \\
                            \hline
                            MONK 3 & 17 -> 4 -> 8 -> 1 & sigmoid & PR & 0.0001 & 0.12 & 0.0
                            & 0.040 - 0.048 & 97 \% - 90 \% \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Results for the Conjugate Gradient Method.}
                \label{tab:monks_cgd}
        \end{table}

        For what concernes the results for the Conjugate Gradient Method, for all the configurations tested, the direction has been set to the modified one.%by eq.

        It's immediate to see the higher performances achieved by the modified $MHS^+$ method and by the $HS^+$ one, both reaching an accuracy of 100\% in the training and test sets. On the contrary, the PR method performes worse than the Stochastic Gradient Descent, behaving as in presence of overfitting. 

        As in the SGD results, the third dataset tested (Monk 3) obtains a little worse performances because of the presence of noise in it.
        Anyway, it's interesting to underline that these models are able to reach higher accuracies with the ability to "auto-tune" themeselves,  without the necessity to modify at hand hyperparameters as the learning rate or the the momentum term. 



        % subsection results_for_conjugate_gradient_method (end)
    % section monks (end)

% chapter experiments (end)
