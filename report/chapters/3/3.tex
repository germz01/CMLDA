\chapter{Experiments} % (fold)
\label{cha:experiments}
    In this final chapter we present the results we obtained by applying our model to the datasets we have used
    to validate and test our ANN, namely, MONKS and CUP. Other than the results, we also present some details
    about the validation phase for each one of the datasets. In appendix \ref{cha:monks_learning_curves} and
    \ref{cha:cup_learning_curves} we added some graphs of the ANN's performances during the experimental phases, in
    order to enrich the presentation.

    \section{MONKS} % (fold)
    \label{sec:monks}
        Before delving into the details of the results we obtained by applying our model to the dataset, we
        provide some informations about the \textit{preprocessing routines} and \textit{validation schema} we
        decided to use. Here are the steps we followed in order to reach the final states of our analysis.

        \begin{enumerate}
            \item Since the MONKS datasets’ feature are categorical, that is, every feature’s value represents
            a class, not a numerical value, we preprocessed the three datasets by writing a script
            for applying a \textit{1-of-k encoding}, hence obtaining 17 binary input features.
            \item As a supplementary preprocessing phase, we have applied a \textit{symmetrization} to the
            matrix containing the dataset’s values, in order to ease the training during the validation phase
            by having a matrix of values closer to the symmetric behavior of the sigmoid function, which was
            introduced in section \ref{sec:the_activation_functions}.
            \item Since we have chosen to follow \cite{Bergstra:2012:RSH:2188385.2188395} for the
            hyperparameters' search during the validation phase, we first performed some
            \textit{preliminary trials} in order to have a glimpse on the best intervals for searching our
            model's hyperparameters. During this trials we manually varied the model's hyperparameters, e.g.
            the learning rate, the momentum constant and so on for the SGD and the rho constant for the CGD,
            and observed the resulting \textit{learning curves}. For this part of the analysis we have used
            the $20\%$ of the training set as validation set, and the remaining part for training the network.
            \item We then deepen the search using the most interesting intervals discovered during the
            preliminary trials in the validation phase by using our implementation of the (random)
            \textit{grid search algorithm}, in which we also used our implementation of the
            \textit{k-fold cross validation algorithm} (which follows the approach of using a value
            of 5 for the k parameter).
        \end{enumerate}

        Our validation schema for the MONKS dataset essentially consists in using the random grid
        search algorithm to investigate some random sampled "points" in the hyperparameters' space, evaluating
        the performances for each one of this points and finally selecting the best combinations of parameters
        based on the diffent metrics like \textit{generalization error}, \textit{accuracy}, \textit{precision},
        \textit{recall} and \textit{f1-score}. In Tab. \ref{tab:hyper_monk} are reported the ranges for the
        hyperparameters involved in the validation phase.

        \begin{table}[H]
          \centering
          \caption{Hyperparameters' ranges for the random grid search algorithm with SGD and CG.}
          \begin{minipage}{.4\textwidth}
              \centering
              \begin{tabular}{| c | c |}
                    \hline
                    Hyperparameters & Ranges\\
                    \hline
                    $\eta$ & $\left [0.6, 0.8 \right ]$ \\
                    \hline
                    $\alpha$ & $[0.5, 0.9]$ \\
                    \hline
                    $\lambda$ & $[0.001, 0.01]$ \\
                    \hline
              \end{tabular}
          \end{minipage}
          \begin{minipage}{.4\textwidth}
              \centering
              \begin{tabular}{| c | c |}
                    \hline
                    Hyperparameters & Ranges\\
                    \hline
                    $\sigma_2$ & $\left [0.1, 0.4 \right ]$ \\
                    \hline
                    $\rho$ & $[0.0, 1.0]$ \\
                    \hline
              \end{tabular}
            \end{minipage}
            \label{tab:hyper_monk}
        \end{table}

        \subsection{Results (no max epochs)} % (fold)
        \label{sub:results_}

            \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c | c | c |}
                            \hline
                            Task & Optimizer & $\sigma_{1}$ & $\sigma_{2}$ & $\rho$ & $\eta$
                            & $\alpha$ & $\lambda$ & MSE (TR - TS) & Accuracy (TR - TS) (\%) \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 1 & SGD (CM) & - & - & - & 0.71 & 0.53 & 0.0 & $4.21e^{-6}$ - $4.23e^{-6}$
                            & 100 \% - 100 \% \\
                            \hline
                            MONK 1 & SGD (NAG) & - & - & - & 0.60 & 0.67 & 0.0 &
                            $1.15e^{-5}$ - $5.17e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            MONK 1 & CGD ($PR^{+}$) & 0.0001 & 0.17 & 0.0 & - & - & - & $0.020$ - $0.040$
                            & 96 \% - 92 \% \\
                            \hline
                            MONK 1 & CGD ($HS^{+}$) & 0.0001 & 0.33 & 0.0 & - & - & - & $0.012$ - $0.040$
                            & 98 \% - 92 \% \\
                            \hline
                            MONK 1 & CGD ($MHS^{+}$) & 0.0001 & 0.20 & 0.31 & - & - & - &
                            $9.62e^{-6}$ - $7.28e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            \hline
                            MONK 2 & SGD (CM) & - & - & - & 0.60 & 0.90 & 0.0 & $8.11e^{-6}$ - $7.98e^{-6}$
                            & 100 \% - 100 \% \\
                            \hline
                            MONK 2 & SGD (NAG) & - & - & - & 0.73 & 0.80 & 0.0 &
                            $9.08e^{-5}$ - $9.73e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            MONK 2 & CGD ($PR^{+}$) & 0.0001 & 0.21 & 0.0 & - & - & - &
                            $5.86e^{-6}$ - $7.28e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            MONK 2 & CGD ($HS^{+}$) & 0.0001 & 0.31 & 0.0 & - & - & - &
                            $1.01e^{-5}$ - $6.30e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 2 & CGD ($MHS^{+}$) & 0.0001 & 0.11 & 0.94 & - & - & - &
                            $5.10e^{-6}$ - $5.10e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            \hline
                            MONK 3 & SGD (CM) & - & - & - & 0.68 & 0.88 & 0.0081 &
                            $2.08e^{-6}$ - $3.28e^{-6}$
                            & 100 \% - 100 \% \\
                            \hline
                            MONK 3 & SGD (NAG) & - & - & - & 0.61 & 0.77 & 0.0019 &
                            $2.19e^{-6}$ - $1.99e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            MONK 3 & CGD ($PR^{+}$) & 0.0001 & 0.22 & 0.0 & - & - & - &
                            $2.28e^{-5}$ - $2.52e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 3 & CGD ($HS^{+}$) & 0.0001 & 0.12 & 0.0 & - & - & - &
                            $8.96e^{-7}$ - $8.92e^{-7}$ & 100 \% - 100 \% \\
                            \hline
                            MONK 3 & CGD ($MHS^{+}$) & 0.0001 & 0.27 & 0.20 & - & - & - &
                            $5.37e^{-6}$ - $5.64e^{-6}$ & 100 \% - 100 \% \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Comparisons between the iterations having no maximal number of epochs. Each one of the
                iterations has been completed with a network having topology 17 -> 4 -> 8 -> 1.}
                \label{tab:monks_no_max_epochs}
            \end{table}

            \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c |}
                            \hline
                            Task & Optimizer & Convergence Epoch & Elapsed Time & LS
                            Iterations & BP Time & LS Time & Dir Time \\
                            \hline
                            MONK 1 & SGD (CM) & 32624 & 96794.43 & - & 7268.37 & - & - \\
                            \hline
                            MONK 1 & SGD (NAG) & 7626 & 24000.93 & - & 1747.60 & - & - \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 1 & CGD ($PR^{+}$) & 733 & 9992.86 & 7 & 1322.85 & 5980.31 & 14.73 \\
                            \hline
                            MONK 1 & CGD ($HS^{+}$) & 1045 & 14121.16 & 8 & 1961.03 & 8330.85 & 22.46 \\
                            \hline
                            MONK 1 & CGD ($MHS^{+}$) & 449 & 5799.25 & 8 & 812.15 & 3441.85 & 7.94 \\
                            \hline
                            \hline
                            MONK 2 & SGD (CM) & 14406 & 52401 & - & 3630.76 & - & - \\
                            \hline
                            MONK 2 & SGD (NAG) & 7555 & 28465.88 & - & 1935.95 & - & - \\
                            \hline
                            MONK 2 & CGD ($PR^{+}$) & 677 & 9657.37 & 7 & 1386.48 & 5573.81 & 14.59 \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 2 & CGD ($HS^{+}$) & 265 & 4364.99 & 8 & 656.74 & 2432.64 & 5.46 \\
                            \hline
                            MONK 2 & CGD ($MHS^{+}$) & 611 & 8729.49 & 8 & 1921.90 & 5011.61 & 10.82 \\
                            \hline
                            \hline
                            MONK 3 & SGD (CM) & 70059 & 206788.14 & - & 15704.64 & - & - \\
                            \hline
                            MONK 3 & SGD (NAG) & 35890 & 100356.26 & - & 7579.01 & - & - \\
                            \hline
                            MONK 3 & CGD ($PR^{+}$) & 5366 & 78435.54 & 8 & 10988.20 & 46321.27 & 119.78 \\
                            \hline
                            MONK 3 & CGD ($HS^{+}$) & 6479 & 86644.81 & 8 & 12181.34 & 50102.83 & 128.91 \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 3 & CGD ($MHS^{+}$) & 2786 & 31860.66 & 8 & 4568.08 & 18768.19 & 44.48 \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Additional time-related statistics. The unit of time that has been used is the
                millisecond.}
                \label{tab:monks_additional_no_max_epochs}
            \end{table}

            In Table \ref{tab:monks_no_max_epochs} we can find the results for the iteration completed without
            constraining the optimizers by imposing a maximal number of epochs. In order to compare the
            results in a similar configuration between the SGD and the CG, we have decided to adopt only the
            \textit{batch} mode for training the ANN. The regularization constant $\lambda$ is used only in
            the third dataset, since Monk 3 is the only one among the three that has noisy samples. As we can
            see the tasks are divided by the kind of dataset that has been used during the iteration, and, for
            each one of the tasks, the network's topology is the same, that is, 17 -> 4 -> 8 -> 1. For each
            task, the results concerning the MSE and the accuracy are collected by taking the mean of the
            output of 10 different iterations. In gray we can see the best optimizer for each one of the
            tasks. In Table \ref{tab:monks_additional_no_max_epochs} we can see some additioanl statistics
            regarding the execution times of the various iterations, as for the other table, the best
            optimizers for each of the tasks are colored in gray.
            In Figure \ref{fig:monks_MSE_all} we can see a comparison between the optimizers, for each one of
            the three tasks.

            \begin{figure}[t!]
                \centering
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/1_mse_all_time.pdf}
                    }
                    \caption{}
                    \label{fig:monks_1_MSE_all}
                \end{subfigure}
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/2_mse_all_time.pdf}
                    }
                    \caption{}
                    \label{fig:monks_2_MSE_all}
                \end{subfigure}
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/3_mse_all_time.pdf}
                    }
                    \caption{}
                    \label{fig:monks_3_MSE_all}
                \end{subfigure}
                \caption{Comparisons between the diffent optimizers without the constrain of setting a maximal
                number of epochs for the iterations. In Figure \ref{fig:monks_1_MSE_all} we can see the
                comparison for MONKS 1, while in Figure \ref{fig:monks_2_MSE_all} we can see the one for
                MONKS 2 and finally in Figure \ref{fig:monks_3_MSE_all} we can see the one for MONKS 3.}
                \label{fig:monks_MSE_all}
            \end{figure}

        % subsection results_ (end)

        \subsection{Results (max epochs 1000)} % (fold)
        \label{sub:results_}

            \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c | c | c |}
                            \hline
                             Task &  Optimizer &  $\sigma_1$ &  $\sigma_2$ &   $\rho$ &   $\eta$
                             &  $\alpha$ &  $\lambda$ & MSE (TR - TS) &   Accuracy (TR - TS) (\%) \\
                             \hline
                            MONK 1 &   SGD (CM) &        - &        - &     - &  0.65 &   0.75 &  0.0 &  9.92e-03 - 1.48e-02 &  99 \% - 99 \% \\
                            \hline
                            MONK 1 &  SGD (NAG) &        - &        - &     - &  0.63 &   0.73 &  0.0 &  2.18e-02 - 3.17e-02 &  97 \% - 96 \% \\
                            \hline
                            MONK 1 &   CGD (PR) &   0.0001 &     0.18 &  0.0 &     - &      - &       - &  1.54e-02 - 4.88e-02 &  97 \% - 90 \% \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 1 &   CGD (HS) &   0.0001 &     0.22 &  0.0 &     - &      - &       - &  2.12e-03 - 9.71e-03 &  100 \% - 98 \% \\
                            \hline
                            MONK 1 &  CGD (MHS) &   0.0001 &     0.13 &  0.86 &     - &      - &       - &  8.97e-03 - 1.92e-02 &  98 \% - 96 \% \\
                            \hline
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 2 &   SGD (CM) &        - &        - &     - &  0.71 &   0.89 &  0.0 &  6.99e-03 - 9.65e-03 &  100 \% - 100 \% \\
                            \hline
                            MONK 2 &  SGD (NAG) &        - &        - &     - &  0.64 &   0.85 &  0.0 &  1.34e-02 - 1.91e-02 &  98 \% - 98 \% \\
                            \hline
                            MONK 2 &   CGD (PR) &   0.0001 &     0.34 &  0.0 &     - &      - &       - &  1.59e-02 - 3.86e-02 &  96 \% - 92 \% \\
                            \hline
                            MONK 2 &   CGD (HS) &   0.0001 &     0.17 &  0.0 &     - &      - &       - &  1.06e-02 - 1.43e-02 &  98 \% - 97 \% \\
                            \hline
                            MONK 2 &  CGD (MHS) &   0.0001 &     0.33 &  0.43 &     - &      - &       - &  4.53e-03 - 1.13e-02 &  99 \% - 98 \% \\
                            \hline
                            \hline
                            MONK 3 &   SGD (CM) &        - &        - &     - &  0.67 &   0.85 &  0.0027 &  1.18e-02 - 1.31e-02 &  98 \% - 98 \% \\
                            \hline
                            MONK 3 &  SGD (NAG) &        - &        - &     - &  0.61 &   0.80 &  0.0036 &  1.67e-02 - 1.98e-02 &  97 \% - 97 \% \\
                            \hline
                            MONK 3 &   CGD (PR) &   0.0001 &     0.19 &  0.0 &     - &      - &       - &  2.07e-02 - 3.51e-02 &  94 \% - 93 \% \\
                            \hline
                            MONK 3 &   CGD (HS) &   0.0001 &     0.36 &  0.0 &     - &      - &       - &  9.03e-03 - 1.95e-02 &  98 \% - 96 \% \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 3 &  CGD (MHS) &   0.0001 &     0.34 &  0.42 &     - &      - &       - &  7.01e-03 - 1.63e-02 &  99 \% - 96 \% \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Comparisons between the iterations having a maximal number of epochs egual to 1000.
                Each one of the iterations has been completed with a network having topology
                17 -> 4 -> 8 -> 1.}
                \label{tab:monks_max_epochs}
            \end{table}

            \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c |}
                            \hline
                            Task &  Optimizer & Convergence Epoch &  Elapsed Time &  LS Iterations &  BP Time &  LS Time &  Dir Time \\
                            \hline
                            MONK 1 &   SGD (CM) &              1000 &       5163.25 &              - &        - &        - &         - \\
                            \hline
                            MONK 1 &  SGD (NAG) &              1000 &       5214.20 &              - &        - &        - &         - \\
                            \hline
                            MONK 1 &   CGD (PR) &               445 &       5055.85 &            6.0 &   514.70 &  2233.17 &     14.00 \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 1 &   CGD (HS) &               240 &       2665.58 &            6.0 &   267.58 &  1164.87 &      7.95 \\
                            \hline
                            MONK 1 &  CGD (MHS) &               298 &       3278.08 &            7.0 &   328.36 &  1436.53 &      9.57 \\
                            \hline
                            \hline
                            MONK 2 &   SGD (CM) &              1000 &       5385.27 &              - &        - &        - &         - \\
                            \hline
                            MONK 2 &  SGD (NAG) &              1000 &       5437.32 &              - &        - &        - &         - \\
                            \hline
                            MONK 2 &   CGD (PR) &               326 &       3648.71 &            5.0 &   362.99 &  1603.26 &     10.17 \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 2 &   CGD (HS) &               149 &       1695.45 &            5.0 &   169.52 &   760.82 &      4.90 \\
                            \hline
                            MONK 2 &  CGD (MHS) &               183 &       2023.54 &            5.0 &   198.10 &   892.13 &      5.83 \\
                            \hline
                            \hline
                            MONK 3 &   SGD (CM) &              1000 &       5304.07 &              - &        - &        - &         - \\
                            \hline
                            MONK 3 &  SGD (NAG) &              1000 &       5353.35 &              - &        - &        - &         - \\
                            \hline
                            \rowcolor[gray]{.9}
                            MONK 3 &   CGD (PR) &               371 &       4149.39 &            5.0 &   418.39 &  1821.40 &     11.66 \\
                            \hline
                            MONK 3 &   CGD (HS) &               377 &       4319.66 &            6.0 &   439.25 &  1891.85 &     12.75 \\
                            \hline
                            MONK 3 &  CGD (MHS) &               389 &       4361.18 &            6.0 &   438.78 &  1906.62 &     12.70 \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Additional time-related statistics. The unit of time that has been used is the
                millisecond.}
                \label{tab:monks_additional_max_epochs}
            \end{table}

            Here we present the results from a series of iterations where the maximal number of epochs has
            been setted to 1000. Similarly to the previous section, in Table \ref{tab:monks_max_epochs} we
            can find the details regarding the optimizers' performances, while in Table
            \ref{tab:monks_additional_max_epochs} there are the details regarding the time-related
            performances. The considerations done for the iterations with no maximal number of epochs are
            valid also for this specific case. In Figure \ref{fig:monks_MSE_all_max_epochs} we can see a
            comparison betweeen the various optimizers.

            \begin{figure}[t!]
                \centering
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/1_mse_all_time_max_epochs_1000.pdf}
                    }
                    \caption{}
                    \label{fig:monks_1_MSE_all_max_epochs}
                \end{subfigure}
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/2_mse_all_time_max_epochs_1000.pdf}
                    }
                    \caption{}
                    \label{fig:monks_2_MSE_all_max_epochs}
                \end{subfigure}
                \begin{subfigure}{0.45\textwidth}
                    \resizebox{\textwidth}{!}{
                        \includegraphics{img/comparisons/3_mse_all_time_max_epochs_1000.pdf}
                    }
                    \caption{}
                    \label{fig:monks_3_MSE_all_max_epochs}
                \end{subfigure}
                \caption{Comparisons between the diffent optimizers with the constrain of setting a
                maximal number of epochs egual to 1000. In Figure \ref{fig:monks_1_MSE_all_max_epochs}
                we can see the comparison for MONKS 1, while in Figure \ref{fig:monks_2_MSE_all_max_epochs}
                we can see the one for MONKS 2 and finally in Figure \ref{fig:monks_3_MSE_all_max_epochs} we
                can see the one for MONKS 3.}
                \label{fig:monks_MSE_all_max_epochs}
            \end{figure}

        % subsection results_ (end)
    % section monks (end)

    \section{CUP} % (fold)
        \label{sec:cup}

        A main difference with respect to the models used for Monk, is the choice of the topology of the Network.
        First of all, in this case, being the final task a regression one in which there are two different values to be predicted, the output layer of the ANN is composed by two neurons of output, each one associated to an identity function.

        Then, after some preliminary trials, we have decided to set the number of the hidden layers to $[16,\ 32]$.

        Of course, as for the Monk dataset, we have performed a validation phase on the CUP dataset, in order to
        discover the best parameters for the network. As described in Sec. \ref{sec:monks}, it has been
        implemented a \textit{3-fold cross validation algorithm} with a (random) \textit{grid search}.
        Table \ref{tab:hyper_cup} shows the ranges in which the searching of the hyperparameters has been carried
        out. Once again, the momentum type chosen is Nesterov, while the direction used in the Conjugate Gradient
        Methods is the modified one.

        \begin{table}[H]
          \centering
          \caption{Hyperparameters' ranges for the random grid search algorithm with SGD and CG.}
          \begin{minipage}{.4\textwidth}
              \centering
              \begin{tabular}{| c | c |}
                    \hline
                    Hyperparameters & Ranges\\
                    \hline
                    $\eta$ & $\left [0.004, 0.2\right ]$ \\
                    \hline
                    $\alpha$ & $[0.5, 0.9]$ \\
                    \hline
                    $\lambda$ & $[0.0003, 0.003]$ \\
                    \hline
              \end{tabular}
          \end{minipage}
          \begin{minipage}{.4\textwidth}
              \centering
              \begin{tabular}{| c | c |}
                    \hline
                    Hyperparameters & Ranges\\
                    \hline
                    $\sigma_2$ & $\left [0.1, 0.4 \right ]$\\
                    \hline
                    $\rho$ & $[0.0, 1.0]$ \\
                    \hline
              \end{tabular}
            \end{minipage}
            \label{tab:hyper_cup}
        \end{table}

        In tables \ref{tab:cup_sgd} and \ref{tab:cup_cgd} are annotated the average results obtained from 10 executions of each one of the best models identified thanks to the validation step.

        \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c |}
                            \hline
                            Model & Topology & Batch size & Activation & $\eta$ & $\alpha$ & $\lambda$
                            & MSE (TR - TS) \\
                            \hline
                            SGD & 10 -> 16 -> 32 -> 2 & batch & identity & 0.084 & 0.79 & 0.0009 & 1.00 - 1.35 \\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Results for the Stochastic Gradient Descent.}
                \label{tab:cup_sgd}
        \end{table}

        \begin{table}[H]
                \centering
                \begin{subtable}{\textwidth}
                    \resizebox{\textwidth}{!}{
                        \begin{tabular}{| c | c | c | c | c | c | c | c |}
                            \hline
                            $\beta$ & Topology & Batch size & Activation & $\sigma_1$ & $\sigma_2$ & $\rho$
                            & MSE (TR - TS) \\
                            \hline
                            $MHS^+$ & 10 -> -> 16 -> 32 -> 2& batch & identity & 0.0001 & 0.27 & 0.29 & 0.97 - 1.50\\
                            \hline
                            $HS^+$  & 10 ->-> 16 -> 32 -> 2& batch & identity & 0.0001 & 0.39 & 0.0
                            & 0.97 - 1.50  \\
                            \hline
                            $PR^+$  & 10 ->-> 16 -> 32 -> 2& batch & identity & 0.0001 & 0.86 & 0.00
                            & 1.15 - 1.41\\
                            \hline
                        \end{tabular}
                    }
                \end{subtable}
                \caption{Results for the Conjugate Gradient Methods.}
                \label{tab:cup_cgd}
        \end{table}

        As for the experiments with the Monk datasets, we attach the learning curves in Appendix \ref{cha:cup_learning_curves}.
% chapter experiments (end)
