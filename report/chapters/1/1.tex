\chapter{The network} % (fold)
\label{cha:the_network}
	\noindent
	In this first chapter, we provide some informations about the Artificial Neural Network we implemented from
	scratch. We'll describe both the network's structure and the algorithm we used in order to make our network
	\textit{learn} from the data used during the testing and validation phases. Finally we'll present the loss
	function we have chosen for our network, and we'll provide and explanation on how it is differentiable. We'll
	use the notation proposed in \cite{Goodfellow-et-al-2016}.

	\section{The network's structure} % (fold)
	\label{sec:the_network_s_structure}
		\noindent
		Since we have to write from scratch an \textit{Artificial Neural Network}, ANN for short, we have
		considered some alternatives before choosing the network's structure. Finally we agreed on a structure
		composed by one \textit{input layer}, two \textit{hidden layers} and one \textit{output layer}. As
		convention in the Machine Learning community, the number of units in the input layer is egual to the number
		of features of the dataset that is used for the learning, validation and testing phases. The two
		hidden layers contain, respectively, four and eight \textit{hidden neurons}, following the convention of
		putting an increasing series of powers of two as number of hidden units per layer. We have decided to put
		one unit in the output layer when performing a \textit{classification task} and two when performing a
		\textit{regression task}. As we have seen studying the papers and books for gathering the necessary
		knowledge for the project, as \cite{Goodfellow-et-al-2016}, \cite{haykin2009neural} and
		\cite{mitchell1997machine}, choosing to consider the network's structure as an \textit{hyperparameter},
		that is, a variable, could lead to a series of difficult choices during the validation phase, so we have
		decided to fix the ANN structure to the one described.
	% section the_network_s_structure (end)

	\section{The back-propagation algorithm} % (fold)
	\label{sec:the_back-propagation_algorithm}
		\noindent
		In order to make the ANN learn, the information, that is, the feature vector $\mathcal{\mathbf{x}}$ taken
		from the data given in input, has to flow from the input layer through the hidden layers and, finally, the
		output layer, giving the approximation $\hat{\mathbf{y}}$ as output. This is called \textit{forward
		propagation}. The \textit{back-propagation algorithm}, as described in \cite{Goodfellow-et-al-2016},
		\cite{haykin2009neural} and \cite{mitchell1997machine}, allow the information to flow backward through the
		network in order to computer the \textit{gradient}.
	% section the_back-propagation_algorithm (end)

	\section{Loss function is differentiable?} % (fold)
	\label{sec:loss_function_is_differentiable_}

	% section loss_function_is_differentiable_ (end)

% chapter the_network (end)
