\chapter{Optimizers} % (fold)
\label{cha:optimizers}
	\section{Stochastic Gradient Descent} % (fold)
	\label{sec:sgd}
	
	% section struttura_della_rete (end)

		\subsection{Momentum} % (fold)
		\label{sec:momentum}
		
		% section forward_e_back_propagation (end)

		\subsection{Regolarizzazione} % (fold)
		\label{sec:regolarizzazione}

	\section{Nonlinear Conjugate Gradient} % (fold)
	\label{sec:nonlinear_conjugate_gradient}
	
		An intresting optimization ables to lead to an improvement of the performances of the Neural Network, is the use of high-order information during the training phase.

		We can, indeed, approximate the \textit{loss function} in a quadratic form, around a given point \textbf{W}(n), using the Taylor approximation:

		 \begin{equation} 
		 	\label{quadratic}
		    \mathcal{E}(\textbf{W} + \Delta\textbf{W}) = \mathcal{E}(\textbf{W})+\textbf{g}\mathcal{E}^T\Delta\textbf{W}+ \frac{1}{2}\Delta\textbf{W}^T\textbf{H}\Delta\textbf{W},
		 \end{equation} 

		where \textbf{H}(n) is the \textit{Hessian} matrix and \textbf{g} the gradient vector.

		Given Eq. \ref{quadratic}, the optimum adjustment to apply to the weights of the network should be 
 		\begin{equation} 
		 	\label{weight_star}
			\Delta\textbf{W}^* = \textbf{H}^{-1}\textbf{g},
		\end{equation} 

		 carrying the weight of the expensive computation of the inverse of the Hessian.

		Here it comes the \textit{Conjugate Gradient} method: it is an iterative second-order optimization method which allows to avoid the computation of the inverse of the Hessian, getting low memory requirements.

		In fact, through this method, the adjustment to the synaptic weights of the network is computed as:
		 \begin{equation} 
		 	\label{weight}
		    \Delta\textbf{W} = \alpha\textbf{d},
		 \end{equation} 
		where $\alpha$ is the learning rate, while \textbf{d} is the new conjugate direction.

		It derives from the steepest-descent algorithm, in which each new direction is ortoghonal with respect to the previous direction found.
		In this case, the choice of orthogonality does not preserve the minimum along the previous directions found. Instead, it involves the continous minimization of the \textit{loss function} at each step, that is also the cause of the common zig-zag pattern. 

		\subsection{Search Direction}
		\label{sub:search_direction}
			The \textit{Conjugate Gradient} method searches for a direction $\textbf{d}_k$ that holds the sequent property:
			\begin{equation} 
			\textbf{d}_k^T\textbf{H}\textbf{d}_{tk-1} = 0 
			\end{equation}, that means it is conjugate to the previous direction $\textbf{d}_{k-1}$. Furthermore, it doesn't need to know all the previous directions, but it only needs the last one, which is why it requires very little storage and computation.

			Even if, with a quadratic function, by ensuring that the gradient along the previous direction does not increase, it keeps the progress obtained so far in the minimization of the loss function, it's worth to underline that this method can be applyed also in case of nonlinear functions: in this case, it should be necessary to restart the process, since there is no assumption that the conjugate directions previously found are still at the minimum of the function. 

			Each new direction it's a linear combination of the steepest descent -\textbf{g} and the previous direction $\textbf{d}_{k-1}$, and it is defined as:
			\begin{equation}
			  \textbf{d}_k=\begin{cases}
			    -\textbf{g}_0, & \text{if $k=0$};\\
			    -\textbf{g}_k + \beta_k\textbf{d}_{k-1}, & \text{otherwise,}
			  \end{cases}
			\end{equation}

			where $\beta_k$ is a scalar, to be determined, that says how much of the previous direction should be added to the newest one. 

			Another modified search direction, that has been proposed by Zang et al., ensures sufficient descent and has been tested in the project:
			\begin{equation}
			 \textbf{d}_{k} = -(1 + \beta_k\frac{\textbf{g}_k^T\textbf{d}_{k}}{\|\textbf{g}_k\|})\textbf{g}_k + \beta_k\textbf{d}_{k-1}.
			\end{equation}

		\subsection{Beta}
		\label{sub:beta}
			What really makes the difference in the computation of the conjugate gradient algorithm, is the choice of the method used to compute the $\beta$.

			Infact, there have been proposed various choices for computing it, each one giving different efficiency and properties.

			The formulas tested in our implementation are three: the Hestenes-Stiefel (HS), the Fletcher-Reeves (FR) and the Polak-Ribier√®re (PR)
			\begin{equation}
			\label{betas}
				 \beta^{HS}_k = \frac{\textbf{g}_k^T(\textbf{g}_k-\textbf{g}_{k-1})}{(\textbf{g}_k-\textbf{g}_{k-1}^T\textbf{d}_{k-1})}, \text{ }
				 \beta^{PR}_k = \frac{\textbf{g}_k^T(\textbf{g}_k-\textbf{g}_{k-1})}{\|\textbf{g}_{k-1}\|^2}, \text{ }
 				 \beta^{FR}_k = \frac{\|\textbf{g}_{k}\|^2}{\|\textbf{g}_{k-1}\|^2}.
			\end{equation}

			One of the properties that must be garanteed, is the global convergence of the method: since, in our network, we are dealing with a nonquadratic loss function, all the methods have been modified in order to ensure the global convergence:
			\begin{equation}
			\label{beta_max}
				 \beta^+ = max\{\beta, 0\}.
			\end{equation}

			This change provides a sort of restart of the algorithm, in case the $\beta$ found is negative. This is equivalent to forget the last search direction and starting again the search starting again from the steepest descent direction.

			The HS and the PR methods have very similar performance, and they are some of the most efficient conjugate gradient methods. 
			However, using those method with nonlinear functions, there's no garancy that the $\textbf{d}_k$ found is always a descent direction: that's why has been decided to use the modified versions \ref{beta_max}, which ensures that the descent property holds.

			For what concernes the FR method, it requires that $\sigma_1 < \sigma_2 < 0.5$ in order to garantee that the Armijo Wolfe conditions (described in the line search procedure of section \ref{sub:line_search}) are satisfied, and it seems to be less efficient and robust than the other methods.





		\subsection{Line Search} % (fold)
		\label{sub:line_search}
			Once computed the new direction \textbf{d} involved in the new point \textbf{W}+$\alpha$\textbf{d}, a line search has to be implemented in order to find the right step size which minimize the loss function.

			The step size $\alpha$ is nothing more than a scalar: the learning rate for the conjugate gradient algorithm, which tells how far is right to move along a given direction. 

			So, fixed the values of the weights \textbf{W} and the descent direction \textbf{d}, the main goal is to find the right value for $\alpha$ that is able to minimize the loss function:

			 \begin{mini} 
			   {\alpha}{\mathcal{ E}(\textbf{W} + \alpha\textbf{d}).}{}{}
			    \end{mini}

			Of course, we have to deal with a tradeoff: we want a good reduction, but we can't spend too much time computing the exact value for the optimum solution. So, the smarter way to get it is to use an inexact line search, that try some candidate step size and accepts the first one satisfying some conditions.  

			This search is performed in two phases: 
			\begin{itemize}
			\item a \textit{bracketing phase}, that finds an initial interval containing a minimizer;
			\item an \textit{interpolation phase} that, given the interval, finds the right step length in it.
			\end{itemize}

			We decided to use one of the most popular line search condition: the \textit{Armijo-Wolfe} condition.

			The search for the better $\alpha$ is led by two condition:
			\begin{itemize}
			\item the \textit{Armijo} one: 
			\begin{equation}
			\mathcal{E}(W_k+\alpha _kd_k)\leq \mathcal{E}(W_k)+\sigma_1\alpha\nabla \mathcal{E}_k^Td_k
			\end{equation}

			which ensure that $\alpha$ gives a sufficient decrease of the objective function, being this reduction proportional to the step length $\alpha$ and the directional derivative $\nabla \mathcal{E}_k^Td_k$.

			The constant $\sigma_1$ has been set $\sigma_1=10^{-4}$, since it is suggested in literature to be quite small.

			\item the \textit{Strong Wolfe} condition:
			\begin{equation}
			|\nabla \mathcal{E}(W_k+\alpha _kd_k)^Td_k|\leq \mathcal{E}(W_k)+\sigma_2|\nabla \mathcal{E}_k^Td_k|
			\end{equation}
			which garantees to choose steps whose size is not too small.

			It is also known as curvature condition and ensures that, moving of a step $\alpha$ along the given direction, the slope of our function if greater than $\sigma_2$ times the original gradient (if the slope is only slightly negative, the function cannot decrease rapidly along that direction, so it's better to stop the search).

			In this case, the constant $\sigma_2$ is equal to 0.1, since a smaller value gives a more accurate line search.
			Futhermore, having choosen the strong condition, which doesn't allow the derivative to be too positive, we are sure that the $\alpha$ found lies close to a stationary point of the function.
			\end{itemize}

			The algorithm satisfing the Strong Wolfe conditions is implemented through three functions, as described in the pseudocodes \ref{linesearch}, \ref{zoom}, \ref{bisection}:  \texttt{line\_search}, \texttt{zoom} and \texttt{interpolate\_alpha}.

			Since two consecutive values may be similar in finite-precision arithmetic, we set a threshold in both the \texttt{line\_search} and  \texttt{interpolate\_alpha} functions, which garantees that the algorithm stops if two values of $\alpha$ are too close or if the maximum number of iterations has been reached.

			The \texttt{line\_search} function try to find and return a good $\alpha$; if it fails, it returns an interval in which continue the searching, invoking the \texttt{zoom} function, which decreases the size of the interval, until it finds and returns a good step length.

			\texttt{Zoom} invokes another function, \texttt{interpolate\_alpha}, which is nothing more than the implementation of a bisection interpolation in order to find a trial $\alpha$ inside the given interval. 



			\begin{algorithm}
			\caption{Line Search}\label{linesearch}
			\begin{algorithmic}[1]
			% \Input {\alpha_1>0, \alpha_{max}}\\
			% \Output {\alpha_*}\\
			\Procedure{line\_search}{}\\
			\State $\alpha_0 \gets \textit{0};$
			\State $i \gets \textit{1};$
			\While {$i \leq max\_iter$} 
			 		\State  $ \text{Evaluate}\mathcal{E}(\alpha_i);$
					\If  { $[\mathcal{E}(\alpha_i) > \mathcal{E}(0) + \sigma_1\alpha_i\nabla\mathcal{E}_0^Td_0] \text{  or  } [\mathcal{E}(\alpha_i)\leq \mathcal{E}(\alpha_{i-1}) \text{  and  } i>1]  $ }
						\State $\alpha_* \gets \textbf{zoom}(\alpha_{i-1},\alpha_{i});$
						\Return $\alpha_*; $
					\EndIf
					\State   $\text{Evaluate } \nabla\mathcal{E}_{i} $
					\If  { $|\nabla\mathcal{E}_{i}| \leq -\sigma_2\nabla\mathcal{E}_0^Td_0 $}
						\State $\alpha_* \gets \alpha_i; $
						\Return $\alpha_*; $
					\EndIf
					\If  { $\nabla\mathcal{E}_{i} \geq 0$ }
						\State $\alpha_* \gets \textbf{zoom}(\alpha_{i},\alpha_{i-1}); $
						\Return $\alpha_*; $
					\EndIf
					\If  { $(|\mathcal{E}_{i} - \mathcal{E}_{i-1}| \leq threshold$ }
						\State $\alpha_* \gets\alpha_{i}$
						\Return $\alpha_*;$
					\EndIf
					\State $\text{Choose  } \alpha_{i+1}  \in (\alpha_i, \alpha_{max});$
					\State $i \gets i+1; $
			\EndWhile
			\EndProcedure
			\end{algorithmic}
			\end{algorithm}



			\begin{algorithm}
			\caption{Zoom}\label{zoom}
			\begin{algorithmic}[1]
			\Procedure{zoom}{}
			\Repeat 
				\State  $ \alpha_j \gets \textit{interpolate\_alpha}(\alpha_{lo}, \alpha_{hi}); $
				\State $ \text{Evaluate }\mathcal{E}(\alpha_j);$
				\If {$[\mathcal{E}(\alpha_j) > \mathcal{E}(0) + \sigma_1\alpha_j\nabla\mathcal{E}_0^Td_0] \text{  or  } [\mathcal{E}(\alpha_j)\leq \mathcal{E}(\alpha_{lo}) $ }
					\State $\alpha_{hi} \gets \alpha_{j};$
				\Else 
					\State $\text{Evaluate } \nabla\mathcal{E}_j^Td_j; $
					\If { $|\nabla\mathcal{E}_j^Td_j| \leq -\sigma_2\nabla\mathcal{E}_0^Td_0$}
						\State $\alpha_* \gets \alpha_j; $
						\Return $\alpha_*; $
					\EndIf
					\If { $\nabla\mathcal{E}_j^Td_j (\alpha_{hi}-\alpha_{lo}) \geq 0 $ }
						\State $\alpha_{hi} \gets \alpha_{lo}; $
					\EndIf
					\If  { $(|\mathcal{E}_{j} - \mathcal{E}_{0}| \leq threshold$ }
						\State $\alpha_* \gets\alpha_{j}$
						\Return $\alpha_*;$
					\EndIf
				\EndIf
				\State $\alpha_{lo} \gets \alpha_{j}; $
			\EndRepeat 
			\EndProcedure
			\end{algorithmic}
			\end{algorithm}


			\begin{algorithm}
			\caption{Interpolate}\label{bisection}
			\begin{algorithmic}[1]
			\Procedure{interpolate\_alpha}{}
			\State $i \gets \textit{1};$
			\While {$i \leq max\_iter$} 
				\State $\alpha_{mid} \gets (\alpha_{hi}-\alpha_{lo})/2$
				\State  $ \text{Evaluate }\mathcal{E}(\alpha_{mid});$
			 	\If  { $[\mathcal{E}(\alpha_{mid}) == 0]  \text{  or  }  [(\alpha_{hi}-\alpha_{lo})/2 < threshold] $ }
						\Return $\alpha_{mid}; $
				\EndIf	
				\State   $\text{Evaluate } \mathcal{E}(\_{mid}alpha_{lo}); $
				\If  { $sign(\mathcal{E}(\alpha_{mid})) == sign(\mathcal{E}(\alpha_{lo}))$ }
					\State $\alpha_{lo} \gets \alpha_{mid}; $
				\Else
					\State $\alpha_{hi} \gets \alpha_{mid}; $
				\EndIf
				\State $i \gets i+1; $
			\EndWhile
			\EndProcedure
			\end{algorithmic}
			\end{algorithm}




		\subsection{Beta} % (fold)
		\label{sub:beta}

		\subsection{Direction} % (fold)
		\label{sub:direction}
		

		% subsection direction (end)
		
		% subsection beta (end)
		
		% subsection line_search (end)
			
			% section nonlinear_conjugate_gradient (end)		
		% section loss_function_is_differentiable_ (end)
% chapter the_network (end)